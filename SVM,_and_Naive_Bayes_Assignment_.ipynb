{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**SVM, and Naive Bayes Assignment**"
      ],
      "metadata": {
        "id": "P4qPoecIti98"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1.What is Information Gain, and how is it used in Decision Trees?**"
      ],
      "metadata": {
        "id": "O5pxJsVmtre_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS. Information Gain is a measure used in the field of information theory and machine learning, particularly in the construction of decision trees. It quantifies the reduction in entropy (or uncertainty) about an outcome, given some additional information. In simpler terms, it tells us how much 'purer' a set of data becomes after splitting it based on a particular attribute.\n",
        "\n",
        "How is it used in Decision Trees?\n",
        "\n",
        "Decision Trees work by recursively splitting the dataset into subsets based on features that provide the most 'information' about the target variable. Information Gain is the criterion used to decide which attribute to split on at each node of the tree.\n",
        "\n",
        "1. Calculate Entropy: First, the entropy of the entire dataset (or a current node) is calculated. Entropy is a measure of impurity or randomness. A high entropy means the data is mixed (e.g., equal number of 'yes' and 'no' classes), while low entropy means the data is relatively pure (e.g., mostly 'yes' or mostly 'no').\n",
        "\n",
        "2. Calculate Information Gain for Each Attribute: For each potential splitting attribute, the algorithm calculates the entropy of the subsets created by splitting on that attribute. Then, it calculates the 'weighted average' entropy of these subsets. The Information Gain for an attribute is the difference between the entropy before the split and the weighted average entropy after the split.\n",
        "\n",
        ". Information Gain (A) = Entropy (Parent) - [Weighted Average Entropy (Children)]\n",
        "\n",
        "3. Select the Best Attribute: The attribute with the highest Information Gain is chosen as the splitting criterion for that node. This is because it reduces the most uncertainty and creates the purest possible child nodes.\n",
        "\n",
        "4. Repeat: This process is repeated recursively for each child node until a stopping condition is met (e.g., all nodes are pure, a maximum depth is reached, or the Information Gain falls below a threshold).\n",
        "\n",
        "In essence, Information Gain guides the Decision Tree algorithm to make the most effective splits, leading to a tree that can accurately classify or predict outcomes.\n",
        "\n"
      ],
      "metadata": {
        "id": "cmhaJ31bt8us"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.What is the difference between Gini Impurity and Entropy?**"
      ],
      "metadata": {
        "id": "LjFwAyH3urXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS. Both Gini Impurity and Entropy are measures of impurity or disorder used in decision tree algorithms (like CART, C4.5) to decide the best split at each node. While they both aim to quantify how mixed a set of data is, they do so using slightly different mathematical formulations and have some practical differences.\n",
        "\n",
        "Here's a breakdown of their differences:\n",
        "\n",
        "1. Definition and Formula:\n",
        "\n",
        "**Entropy:**\n",
        "\n",
        "Entropy, borrowed from information theory, measures the unpredictability or randomness of a dataset. Higher entropy means more uncertainty or a more mixed dataset.\n",
        "\n",
        "Formula: Entropy(S) = - Σ [p(i) * log2(p(i))]\n",
        "\n",
        "S is the dataset.\n",
        "\n",
        "p(i) is the proportion of observations belonging to class i in the dataset.\n",
        "\n",
        "The summation is over all classes.\n",
        "\n",
        "**Gini Impurity:**\n",
        "\n",
        " Gini Impurity measures the probability of misclassifying a randomly chosen element in the dataset if it were randomly labeled according to the class distribution in the subset. Lower Gini Impurity means less chance of misclassification and a purer subset.\n",
        "\n",
        "Formula:\n",
        "\n",
        "Gini(S) = 1 - Σ [p(i)^2]\n",
        "\n",
        "S is the dataset.\n",
        "\n",
        "p(i) is the proportion of observations belonging to class i in the dataset.\n",
        "\n",
        "The summation is over all classes.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "31z6KYVEu2Hj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.:What is Pre-Pruning in Decision Trees?**"
      ],
      "metadata": {
        "id": "moDU2M8YwSkR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS. Pre-pruning, also known as early stopping, is a technique used in decision tree algorithms to prevent overfitting. It involves stopping the tree construction early, before it has perfectly classified the training data, by setting certain stopping criteria."
      ],
      "metadata": {
        "id": "jeD4fKEfwY3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4:Write a Python program to train a Decision Tree Classifier using Gini\n",
        "Impurity as the criterion and print the feature importances (practical).**\n"
      ],
      "metadata": {
        "id": "RwdS0g-Xws5N"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "b47b1300",
        "outputId": "c390640b-3c1e-4dce-d780-e27c54fbed67"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset as an example\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier with Gini Impurity\n",
        "dtree_gini = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "dtree_gini.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = dtree_gini.feature_importances_\n",
        "\n",
        "# Create a pandas Series for better readability\n",
        "feature_importances = pd.Series(importances, index=feature_names)\n",
        "\n",
        "print(\"Feature Importances (Gini Impurity):\")\n",
        "display(feature_importances.sort_values(ascending=False))\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances (Gini Impurity):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "petal length (cm)    0.893264\n",
              "petal width (cm)     0.087626\n",
              "sepal width (cm)     0.019110\n",
              "sepal length (cm)    0.000000\n",
              "dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>petal length (cm)</th>\n",
              "      <td>0.893264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>petal width (cm)</th>\n",
              "      <td>0.087626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sepal width (cm)</th>\n",
              "      <td>0.019110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sepal length (cm)</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5.What is a Support Vector Machine (SVM)?**"
      ],
      "metadata": {
        "id": "2fFAc5RQxM_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS. A Support Vector Machine (SVM) is a powerful and versatile machine learning algorithm used for both classification and regression tasks. However, it's primarily known for its effectiveness in classification, particularly for solving two-class (binary) classification problems.\n",
        "\n",
        "At its core, an SVM works by finding the optimal hyperplane that best separates data points belonging to different classes in a high-dimensional space."
      ],
      "metadata": {
        "id": "r3bMNr32xLNf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6.What is the Kernel Trick in SVM?**"
      ],
      "metadata": {
        "id": "RwM7xKMyxn5n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS. The Kernel Trick is a fundamental concept that allows Support Vector Machines (SVMs) to effectively handle non-linearly separable data. It's a very clever mathematical technique that avoids the explicit transformation of data into higher-dimensional spaces, saving a tremendous amount of computational cost."
      ],
      "metadata": {
        "id": "gI8-RyXoxvjL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7.Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.**"
      ],
      "metadata": {
        "id": "y-Z8kpjHyFsD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46167b4f",
        "outputId": "d00ee584-ddfa-4012-c9fa-bc945c2ead6c"
      },
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 1. Train SVM with Linear Kernel\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate accuracy for Linear SVM\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "print(f\"Accuracy with Linear Kernel: {accuracy_linear:.4f}\")\n",
        "\n",
        "# 2. Train SVM with RBF (Gaussian) Kernel\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate accuracy for RBF SVM\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "print(f\"Accuracy with RBF Kernel: {accuracy_rbf:.4f}\")\n",
        "\n",
        "# Compare accuracies\n",
        "if accuracy_linear > accuracy_rbf:\n",
        "    print(\"\\nLinear Kernel performed better.\")\n",
        "elif accuracy_rbf > accuracy_linear:\n",
        "    print(\"\\nRBF Kernel performed better.\")\n",
        "else:\n",
        "    print(\"\\nBoth kernels performed equally.\")\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Linear Kernel: 0.9815\n",
            "Accuracy with RBF Kernel: 0.7593\n",
            "\n",
            "Linear Kernel performed better.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8 What is the Naïve Bayes classifier, and why is it called \"Naïve\"?**"
      ],
      "metadata": {
        "id": "W1faSDgFyu2a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS. The Naïve Bayes classifier is a family of algorithms that apply Bayes' Theorem with the \"naïve\" assumption of conditional independence between every pair of features given the value of the class variable. Despite its simplistic assumption, Naïve Bayes often performs surprisingly well in practice, especially for text classification and spam detection.\n",
        "\n",
        "Bayes' Theorem states:\n",
        "\n",
        "P(A|B) = [P(B|A) * P(A)] / P(B)\n",
        "\n",
        "\n",
        "\n",
        "**Why is it called \"Naïve\"?**\n",
        "\n",
        "\n",
        "The \"Naïve\" part comes from its fundamental assumption: that all features in the input vector are conditionally independent of each other, given the class.\n",
        "\n",
        "In other words, it assumes that the presence or absence of one feature does not affect the presence or absence of any other feature, given that we know the class. Mathematically, this means:\n",
        "\n",
        "P(x_1, x_2, ..., x_n | y) = P(x_1 | y) * P(x_2 | y) * ... * P(x_n | y)"
      ],
      "metadata": {
        "id": "ek2cQvuQy4cW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9.Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes?**\n"
      ],
      "metadata": {
        "id": "k5Qaj9ADzmjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS. **1. Gaussian Naïve Bayes**\n",
        "\n",
        "**Best Suited for:** Continuous data. Features are assumed to follow a Gaussian (normal) distribution.\n",
        "\n",
        "**Likelihood Calculation:** It assumes that the continuous values associated with each feature are distributed according to a Gaussian distribution. When calculating the likelihood P(x_i | y), it estimates the mean (μ_y) and variance (σ²_y) of feature x_i for each class y from the training data. Then, it uses the probability density function (PDF) of the Gaussian distribution to compute the likelihood for a given x_i.\n",
        "\n",
        "**Example Use Case:** Classifying based on features like height, weight, temperature, or other real-valued measurements.\n",
        "\n",
        "**Mathematical Form of**\n",
        " P(x_i | y): P(x_i | y) = (1 / sqrt(2 * π * σ²_y)) * exp(-(x_i - μ_y)² / (2 * σ²_y))\n",
        "\n",
        "**2. Multinomial Naïve Bayes**\n",
        "**Best Suited for:**\n",
        "Discrete counts. This classifier is typically used for features that represent the frequencies with which certain events have been generated by a multinomial distribution. It's especially popular for text classification.\n",
        "\n",
        "**Likelihood Calculation:**\n",
        "\n",
        "For each feature (e.g., a word in a document) and each class, it calculates the probability of observing that feature count given the class. It uses a multinomial distribution where the likelihood is proportional to the count of feature x_i in documents of class y, normalized by the total count of all features in documents of class y.\n",
        "\n",
        "**Example Use Case:**\n",
        "Text classification (e.g., spam detection, sentiment analysis), where features are typically word counts or term frequencies within a document. For instance, if you're classifying emails as spam or not spam, the features might be the counts of words like \"money,\" \"free,\" or \"Viagra.\"\n",
        "**Mathematical Form of**\n",
        "P(x_i | y): P(x_i | y) = (count(x_i, y) + α) / (Σ_k count(x_k, y) + α * V) Where count(x_i, y) is the number of times feature x_i appears in samples of class y, α is a smoothing parameter (for Laplace or Lidstone smoothing to handle zero probabilities), and V is the total number of unique features.\n",
        "\n",
        "**3. Bernoulli Naïve Bayes**\n",
        "Best Suited for: Binary or boolean features. This classifier assumes that features are independent Bernoulli distributed variables. This means each feature can either be present (1) or absent (0).\n",
        "\n",
        "\n",
        "Likelihood Calculation: For each feature and each class, it estimates the probability that a feature x_i is present (P(x_i=1 | y)) and the probability that it is absent (P(x_i=0 | y)).\n",
        "\n",
        "Example Use Case: Document classification where features are not word counts, but rather indicators of whether a specific word is present or absent in a document. For example, a document either contains the word \"sale\" or it doesn't.\n",
        "\n",
        "\n",
        "Mathematical Form of P(x_i | y): P(x_i | y) = P(x_i=1 | y) if x_i is 1 (present) P(x_i | y) = 1 - P(x_i=1 | y) if x_i is 0 (absent) Where P(x_i=1 | y) is the probability that feature x_i is present in samples of class y.\n",
        "Summary Table:\n",
        "\n",
        "Classifier\tData Type\tFeature Modeling Assumption\tCommon Use Case\n",
        "Gaussian NB\tContinuous\tFeatures follow a normal distribution.\tNumerical data (e.g., sensor readings, heights)\n",
        "Multinomial NB\tDiscrete Counts\tFeatures represent event counts (multinomial).\tText classification (word counts)\n",
        "Bernoulli NB\tBinary/Boolean\tFeatures are binary (present/absent).\tDocument classification (word presence)\n",
        "The choice of which Naïve Bayes variant to use depends entirely on the nature of your features in the dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "Fyg_MFq30BAf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10  Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.**\n",
        "\n"
      ],
      "metadata": {
        "id": "73tJATh12Uqb"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "def7dd44",
        "outputId": "7b8ab6af-7331-4526-dc49-6022d2b7cbf4"
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train the classifier\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy of Gaussian Naïve Bayes on the Breast Cancer dataset: {accuracy:.4f}\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Gaussian Naïve Bayes on the Breast Cancer dataset: 0.9415\n"
          ]
        }
      ]
    }
  ]
}